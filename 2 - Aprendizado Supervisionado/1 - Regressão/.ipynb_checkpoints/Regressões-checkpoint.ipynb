{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressões\n",
    "\n",
    "## Regressão Linear Simples\n",
    "\n",
    "Vamos começar inicialmente com a regressão linear simples. Note que existe o arquivo Salary_Data.csv nesta pasta, que é o arquivo que vamos trabalhar neste módulo.\n",
    "\n",
    "Iremos realizar técnicas de pré-processamento de dados (modificações simples, o foco é em ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "# Importando as bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset\n",
    "dataset = pd.read_csv('Salary_Data.csv')\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em conjunto de treino e conjunto de testes\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de Regressão Linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression(normalize = True)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizendo no conjunto de testes\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"R2 de treino: {}\".format(r2_train))\n",
    "print(\"R2 de teste: {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando o modelo no conjunto de treino\n",
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, y_pred_train , color = 'blue')\n",
    "plt.title('Salary vs Experience (Training set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando o modelo no conjunto de testes\n",
    "plt.scatter(X_test, y_test, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (Test set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Múltipla\n",
    "\n",
    "Regressão Linear múltipla é semelhante à regressão linear simples, porém com mais de uma variável (regressor). O raciocínio (e o algoritmo) são os mesmos. Mudaremos apenas o dataset (50_Startups.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratando categorical data\n",
    "X = pd.get_dummies(data = X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evitando a Dummy Variable Trap\n",
    "X.drop('State_California', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em conjunto de treino e testes\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de regressão linear múltipla\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizendo os resultados no test set\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### F Test\n",
    "Iremos utilizar f_regression para implementar o método de seleção de features F Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando F e pval\n",
    "from sklearn.feature_selection import f_regression\n",
    "F, pval = f_regression(X_train, y_train)\n",
    "\n",
    "# Excluindo features com baixo pval\n",
    "X_train_ftest = X_train.iloc[:, pval > 0.05]\n",
    "X_test_ftest = X_test.iloc[:, pval > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ftest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information \n",
    "\n",
    "Iremos utilizar mutual_info_regression para calcular implementar o método de seleção de features de Informação Mútua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "mi = mutual_info_regression(X_train, y_train, random_state = 42)\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluindo features com baixo mi\n",
    "X_train_mi = X_train.iloc[:, mi > 0.5]\n",
    "X_test_mi = X_test.iloc[:, mi > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "\n",
    "Iremos utilizar RFE para calcular implementar o método de seleção de features de Backwards Elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X_train_rfe = RFE(estimator = regressor, n_features_to_select = 3)\n",
    "X_train_rfe.fit(X_train, y_train)\n",
    "X_train.iloc[:, X_train_rfe.support_].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como conclusão, para este modelo específico, nota-se que a localização da startup (Flórida ou New York), e também a quantia gasta em R&D são as melhores features para aplicar ao modelo de regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Polinomial\n",
    "\n",
    "Regressão polinomial é realizada quando existe uma dependência polinomial entre seus dados e seus rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2]\n",
    "y = dataset.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o intuito de visualização e teste, criaremos uma regressão linear em cima dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Visualização do modelo\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X, lin_reg.predict(X), color = 'blue')\n",
    "plt.title('Level x Salary (Linear Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é possível perceber, uma reta não é o melhor modelo para representar estes dados. A solução é adicionar termos polinomiais para melhor adequação da nossa curva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de regressão polinomial - fornecer features polinomiais para LR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree = 4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "\n",
    "# criando regressão polinomial\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos perceber que existem features polinomiais nos nossos dados. Desta forma, basta aplicarmos LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaçãoo do modelo de regressção polinomial\n",
    "\n",
    "#X_grid = np.arange(min(X), max(X), 0.1)\n",
    "#X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X, lin_reg_2.predict(X_poly), color = 'blue')\n",
    "plt.title('Level x Salary (Polynomial Regression)')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Regression\n",
    "\n",
    "Árvores de decisão são utilizadas tanto para regressão, quanto para classificação. É muito comum ouvir o termo CART (Classification and Regression Trees). É um algoritmo intuitivo, onde o objetivo é criar um modelo que prediz um valor baseado em pequenas regras de decisão inferidas das features.\n",
    "\n",
    "\n",
    "### Vantagens:\n",
    "- Simples de entender e interpretar;\n",
    "- Lida bem com dados numéricos e categóricos;\n",
    "- Custo computacional logarítmico no número training examples.\n",
    "\n",
    "### Desvantagens:\n",
    "- Podem criar árvores muito complexas que não generalizam bem (overfitting);\n",
    "- Instável, pequenas variações nos dados podem causar árvores completamente diferentes (problema resolvido com ensemble);\n",
    "- É considerado um algoritmo NP-completo sob a ótima de otimização, usualmente os nós são otimizados encontrando-se mínimos locais (problema resolvido com ensemble);\n",
    "- Muito influenciado por bias;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(criterion = 'mse', random_state = 42)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização do modelo\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X, regressor.predict(X), color = 'blue') # <- modelo de baixa resolução\n",
    "plt.title('Level x Salary (Decision Trees Regression)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização com maior resolução\n",
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Level x Salary (Decision Trees Regression)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos um outro exemplo, onde tentamos aproximar uma função senoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dados aleatórios\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as árvores\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando os resultados\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\",label=\"max_depth = 2\")\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth = 5\")\n",
    "plt.xlabel(\"Dados\")\n",
    "plt.ylabel(\"Rótulos\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É notável que para max_depth = 2 houve um fenômeno chamado Underfitting (ou High Bias), onde o modelo não é flexível o suficiente para generalizar bem para novos dados. Com max_depth = 5, temos um evento contrário à este, chamado Overfitting (ou High Variance), onde o modelo se dobra demais, também generalizando mal para novos dados.\n",
    "\n",
    "<img src=\"overfit.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random Forest é um algoritmo do tipo ensemble. Algoritmos Ensemble são conhecidos por serem robustos à overfitting e por performarem bem para dados de teste, uma vez que utilizam modelos mais fracos (como árvores de decisão) para compor um modelo mais poderoso. \n",
    "\n",
    "Usualmente usam-se centenas de árvores de decisão para um ensemble de Random Forest. Também podem ser usados para classificação, e costumam extrair relações complexas entre os dados, criando árvores e mais árvores capazes de armazenar as decisões para dados com bastante densidade.\n",
    "\n",
    "O processo de agrupamento dos resultados das árvores de decisão costuma anular o efeito de overfitting (semelhante à um sinal ruidoso, onde é possível cancelar o rúido pelo cálculo da média do sinal). Assim, tem-se um dos algoritmos mais poderosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(criterion = 'mse', n_estimators = 1000, random_state = 0, verbose = 2)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização do Random Forest Regression \n",
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Level x Salary (Random Forest Regression)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, pode-se perceber que o modelo foi melhorado. A diferença não é tão significante devido à natureza e simplicidade dos dados, porém para dados mais desafiadores este modelo costuma performar bem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
