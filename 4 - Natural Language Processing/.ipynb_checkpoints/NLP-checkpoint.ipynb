{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Problemas de processamento natural de linguagem (ou text mining) são extremamente comuns e vivem em constante melhoria. É óbvio que o computador não é capaz de ler uma frase como nós, por isso é necessário transformar o corpo do texto em algo numérico (modelo bag of words vetorizado). Para isso, usam-se algumas técnicas que serão descritas neste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset - quoting 3 = ignorando aspas duplas\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Existem alguns métodos de vetorização, dentre eles:\n",
    "- CountVectorizer\n",
    "- Tf-IDf\n",
    "- Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpando o texto\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # stopwords são preposições, 'this', 'that',...\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #loving = love\n",
    "corpus = [] # inicializando como uma lista varia\n",
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) #substituindo tudo que não for letras por espaços em branco\n",
    "    review = review.lower() # transformando em letras minúsculas\n",
    "    review = review.split() # criando um vetor de palavras\n",
    "    ps = PorterStemmer() # inicializando a classe stem\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # iterando pelo vetor de palavras e excluindo as stopwords\n",
    "    review = ' '.join(review) # juntando novamente em string\n",
    "    corpus.append(review) # append no corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "\n",
    "vectorize_method = 'tfidf'\n",
    "\n",
    "if (vectorize_method == \"tfidf\"):\n",
    "    tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,sublinear_tf=True)\n",
    "    X = tfidf.fit_transform(corpus).toarray()\n",
    "elif (vectorize_method == \"cv\"):\n",
    "    cv = CountVectorizer()\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "elif (vectorize_method == \"hv\"):\n",
    "    hv = HashingVectorizer(decode_error='ignore', n_features=2 ** 14) \n",
    "    X = hv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em conjunto de treino e teste\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de classificação\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevendo no conjunto de testes\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a matriz de confusão\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot = True)\n",
    "plt.show()\n",
    "print('Acurácia: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
